{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self defined methods\n",
    "from src.data.data_preprocessing import *\n",
    "from src.label.make_dataset import *\n",
    "from src.label.product_graph import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and clean original Amazon datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original datasets are in JSON format. They have to be cleaned up and parsed into dataframe. Codes, which were used in this project to parse original data, are based on the colab notbook that the author of the datasets provided: https://colab.research.google.com/drive/1Zv6MARGQcrBbLHyjPVVMZVnRWsRnVMpV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the following codes you have to download the original datasets that could be find on this page: http://deepyeti.ucsd.edu/jianmo/amazon/index.html and save them in OriginalData folder. You can download any category of you choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_if_not_exists(name):\n",
    "    if not os.path.exists(name):\n",
    "        os.makedirs(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary provides aliases for original data file names. For this project we used the following categories. \n",
    "# Replace them with the categories of your choice. \n",
    "datasets = {'art': 'meta_Arts_Crafts_and_Sewing',\n",
    " 'pet': 'meta_Pet_Supplies',\n",
    " 'home': 'meta_Home_and_Kitchen',\n",
    " 'garden': 'meta_Patio_Lawn_and_Garden',\n",
    " 'sport': 'meta_Sports_and_Outdoors',\n",
    " 'toy': 'meta_Toys_and_Games',\n",
    " 'tool': 'meta_Tools_and_Home_Improvement'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the downloaded original files, parsed them into pandas dataframes then cleaned up the data.\n",
    "# Cleaned data will be save into FilteredData folder for later use.\n",
    "# This process could take some time to finish.\n",
    "output = \"FilteredData\"\n",
    "make_dir_if_not_exists(output)\n",
    "for data in datasets:\n",
    "    df = parse_dataset(f'OriginalData/{datasets[data]}.json.gz')\n",
    "    df = parse_column(df)\n",
    "    df.to_pickle(f'FilteredData/{datasets[data]}_df_rm_duplicate_clean_similar_item.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of filtered data\n",
    "df=pd.read_pickle(\"FilteredData/meta_Arts_Crafts_and_Sewing_df_rm_duplicate_clean_similar_item.pickle\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate pair of similar and not similar products and triplets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define extra functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(ID, df):\n",
    "    \"\"\"Download the image that belongs to the given product ID (ASIN) then save it into Images folder.\n",
    "\n",
    "    Parameters:\n",
    "    ID (str): ASIN of products whose image should be downloaded\n",
    "    df (pandas.Dataframe): cleaned dataframe of one product category\n",
    "\n",
    "   \"\"\"\n",
    "    url = df[df.asin==ID]['image'].values[0][0]\n",
    "    r = urllib.request.urlopen(url)\n",
    "    with open(f'images/{ID}.jpg', 'wb') as handler:\n",
    "        handler.write(r.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(num, connected_components):\n",
    "    \"\"\"Returns a list of tuples of 2 similar products. A pair of products will be considered as similar\n",
    "    if they are in the same connected components.\n",
    "\n",
    "    Parameters:\n",
    "    num (int): Number of matches that should be generated\n",
    "    connected_components (nested list of strings): Generated connected components from product graph\n",
    "\n",
    "    Returns:\n",
    "    Dict<(str, str),int>: Dictionary with key is a tuple of 2 ASINs that presents 2 similar products and values is\n",
    "    the constant integer 1.\n",
    "\n",
    "   \"\"\"\n",
    "    # loaded_image_ID is a global defined object that contains all downloaded images.\n",
    "    global loaded_image_ID\n",
    "    matches={}; edges=[] \n",
    "    # Flatten the netsted list of connected components\n",
    "    connected_components = flatten(connected_components)\n",
    "    # Save all edges in the components into one lists. Only products with direct connection will be drawn.\n",
    "    for c in connected_components: \n",
    "        edges.extend(list(c.edges))\n",
    "    while True:\n",
    "        # Draw a random pair of directly connected products\n",
    "        m1, m2 = edges[np.random.choice(len(edges))]\n",
    "        # Download their images\n",
    "        if m1 not in loaded_image_ID:            \n",
    "            try:\n",
    "                get_image(m1, df)\n",
    "                loaded_image_ID[m1]=1\n",
    "            except: print(m1, ' not loaded'); continue\n",
    "        if m2 not in loaded_image_ID:            \n",
    "            try:\n",
    "                get_image(m2, df)\n",
    "                loaded_image_ID[m2]=1\n",
    "            except: print(m2, ' not loaded'); continue\n",
    "        matches[(m1,m2)]=1\n",
    "        # If the desired number of pairs of similar products are reached, the processed will stop.\n",
    "        if len(matches)==num: \n",
    "            break\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mismatches(num, connected_components):\n",
    "    \"\"\"Returns a list of tuples of 2 not similar products. A pair of products will be considered as not similar\n",
    "    if they are in the same connected components.\n",
    "\n",
    "    Parameters:\n",
    "    num (int): Number of matches that should be generated\n",
    "    connected_components (nested list of strings): Generated connected components from product graph\n",
    "\n",
    "    Returns:\n",
    "    Dict<(str, str),int>: Dictionary with key is a tuple of 2 ASINs that presents 2 not similar products and values is\n",
    "    the constant integer 1.\n",
    "\n",
    "   \"\"\"\n",
    "    # loaded_image_ID is a global defined object that contains all downloaded images.\n",
    "    global loaded_image_ID\n",
    "    mismatches={}\n",
    "    # Remove category with only one subcategory\n",
    "    connected_components = [i for i in connected_components if len(i)>1]\n",
    "    sub_cats = [[list(cc.nodes) for cc in sub_cat] for sub_cat in connected_components]\n",
    "    while True:\n",
    "        sub_cat = sub_cats[np.random.choice(len(sub_cats))]\n",
    "        # Choose 2 random products that belong to 2 different connected component in the same subcategory\n",
    "        component_1, component_2 = np.random.choice(len(sub_cat), size=2, replace=False)\n",
    "        component_1, component_2 = sub_cat[component_1], sub_cat[component_2]\n",
    "        node_1, node_2 = component_1[np.random.choice(len(component_1))], component_2[np.random.choice(len(component_2))]  \n",
    "        # Download their images\n",
    "        if node_1 not in loaded_image_ID:\n",
    "            try:\n",
    "                loaded_image_ID[node_1]=1\n",
    "                get_image(node_1)\n",
    "            except:\n",
    "                continue\n",
    "        if node_2 not in loaded_image_ID:\n",
    "            try:\n",
    "                loaded_image_ID[node_2]=1\n",
    "                get_image(node_2)\n",
    "            except:\n",
    "                continue\n",
    "        mismatches[(node_1, node_2)]=1\n",
    "        if len(mismatches)>=num:\n",
    "            break\n",
    "    return mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_triplet(num, connected_components):\n",
    "    \"\"\"Returns a list of product triplets (anchor, positive and negative)\n",
    "\n",
    "    Parameters:\n",
    "    num (int): Number of triplets that should be generated\n",
    "    connected_components (nested list of strings): Generated connected components from product graph\n",
    "\n",
    "    Returns:\n",
    "    list[(str, str, str)]: List of triplets.\n",
    "\n",
    "   \"\"\"\n",
    "    # loaded_image_ID is a global defined object that contains all downloaded images.\n",
    "    global loaded_image_ID\n",
    "    # Remove category with only one subcategory\n",
    "    triplets = {}\n",
    "    connected_components = [i for i in connected_components if len(i)>1]\n",
    "    sub_cats = [[list(cc.edges) for cc in sub_cat] for sub_cat in connected_components]\n",
    "    while True:\n",
    "        sub_cat = sub_cats[np.random.choice(len(sub_cats))]\n",
    "        component_1, component_2 = np.random.choice(len(sub_cat), size=2, replace=False)\n",
    "        component_1, component_2 = sub_cat[component_1], sub_cat[component_2]\n",
    "        edge_1, edge_2 = component_1[np.random.choice(len(component_1))], component_2[np.random.choice(len(component_2))]\n",
    "        node_1, node_2, node_3 = edge_1[0], edge_1[1], edge_2[0]\n",
    "        if node_1 not in loaded_image_ID:\n",
    "            try:\n",
    "                loaded_image_ID[node_1]=1\n",
    "                get_image(node_1)\n",
    "            except:\n",
    "                continue\n",
    "        if node_2 not in loaded_image_ID:\n",
    "            try:\n",
    "                loaded_image_ID[node_2]=1\n",
    "                get_image(node_2)\n",
    "            except:\n",
    "                continue\n",
    "        if node_3 not in loaded_image_ID:\n",
    "            try:\n",
    "                loaded_image_ID[node_3]=1\n",
    "                get_image(node_3)\n",
    "            except:\n",
    "                continue\n",
    "        triplets[(node_1, node_2, node_3)]=1        \n",
    "        if len(triplets)>=num:\n",
    "            break\n",
    "    return list(triplets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate traning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_img = \"Images\"\n",
    "make_dir_if_not_exists(output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_image_ID is a global defined object that contains all downloaded images.\n",
    "# It should be empty at first.\n",
    "loaded_image_ID={}\n",
    "for filename in os.listdir(output_img):\n",
    "    loaded_image_ID[filename]=1\n",
    "print(len(loaded_image_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_all_cat = \"TrainingData/AllCategories/\"\n",
    "output_single_cat = \"TrainingData/SingleCategory/\"\n",
    "make_dir_if_not_exists(output_all_cat)\n",
    "make_dir_if_not_exists(output_single_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_dataset=[]\n",
    "img_triplets = []\n",
    "for ds in datasets:\n",
    "    print(datasets[ds])\n",
    "    # Read parsed dataset\n",
    "    df = pd.read_pickle(f'FilteredData/{datasets[ds]}_df_rm_duplicate_clean_similar_item.pickle')\n",
    "    # Remove products with missing relevant informations\n",
    "    df_cleaned = clean_dataframe(df, [\"title\",\"feature\",\"description\",\"category\",\"similar_item\", \"image\"])\n",
    "    # Make connected components bases on the original dataframe then remove the nodes that are not in the cleaned one.\n",
    "    connected_components = df_component(df, df_cleaned)\n",
    "    # Generate similar products pair. You can specify the number you wish to have.\n",
    "    matches = get_matches(12, connected_components)\n",
    "    print(len(matches), ' pairs of similar products.')\n",
    "    mismatches = get_mismatches(30, connected_components)\n",
    "    print(len(mismatches), ' pairs of not similar products.')\n",
    "    # Convert the product attributs into one string\n",
    "    data = serialize(df_cleaned, matches, mismatches)\n",
    "    serialized_dataset.extend(data)\n",
    "    # Generate product triplets\n",
    "    triplets = get_image_triplet(20, connected_components)\n",
    "    img_triplets.extend(triplets)\n",
    "    # Write training data for single category\n",
    "    write_training_data(f'{output_single_cat}{ds_}', serialized_dataset)\n",
    "    write_triplets_data(f'{output_single_cat}{ds_}', img_triplets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix data of all categories then write in one single training dataset\n",
    "write_training_data(output_all_cat, serialized_dataset)\n",
    "write_triplets_data(output_all_cat, img_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac3a4f718b317d0f59de664fd35a817365df8aa5069a306a5d5a85d818aebd22"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
